{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet50.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM4toeQqg+40Y/N8sl7mC/C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedbakhet/Medicine-In-Take-Reminder-and-Monitor/blob/master/ResNet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sROmJQmL9Y8C"
      },
      "source": [
        "! pip install tensorflow\n",
        "! pip install h5py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuHwlbWC9wEg"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from keras.layers import Input\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import Sequential,models\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten,MaxPooling2D,Conv1D,MaxPooling1D,add,Activation,GlobalAveragePooling2D,BatchNormalization,ReLU,MaxPool2D,SeparableConv2D,Add,GlobalAvgPool2D\n",
        "#from keras.utils import plot_model\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import os,keras\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_BFtir492-j"
      },
      "source": [
        "#load train data set\n",
        "training_data=[]\n",
        "for index,value in enumerate (os.listdir('/content/drive/MyDrive/train_data')): #path to the data set on google drive\n",
        "  print(value)\n",
        "  lis=np.load('/content/drive/MyDrive/train_data/'+value)\n",
        "  for j in lis[:1600]:\n",
        "    training_data.append([np.array(j),value[:-4]])\n",
        "shuffle(training_data)\n",
        "\n",
        "#load test data set\n",
        "test_data=[]\n",
        "for index,value in enumerate (os.listdir('/content/drive/MyDrive/train_data')):\n",
        "  print(value)\n",
        "  lis=np.load('/content/drive/MyDrive/train_data/'+value)\n",
        "  for j in lis[:400]:\n",
        "    test_data.append([np.array(j),value[:-4]])\n",
        "shuffle(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCwKcqRX93JI"
      },
      "source": [
        "#split data into train_test & encoding target\n",
        "\n",
        "X_train = np.array([i[0] for i in training_data]).reshape(-1,300,15,1)\n",
        "Y_train = np.array([i[1] for i in training_data])\n",
        "train_labels_set = np.unique(Y_train,axis=0)\n",
        "train_labels = LabelEncoder()\n",
        "train_labels = train_labels.fit(Y_train)\n",
        "Y_train = to_categorical(train_labels.transform(Y_train))\n",
        "\n",
        "X_test = np.array([i[0] for i in test_data]).reshape(-1,300,15,1)\n",
        "Y_test = np.array([i[1] for i in test_data])\n",
        "test_labels_set = np.unique(Y_test,axis=0)\n",
        "test_labels = LabelEncoder()\n",
        "test_labels = test_labels.fit(Y_test)\n",
        "Y_test = to_categorical(test_labels.transform(Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivQsa1jc-5eM"
      },
      "source": [
        "#ResNet50\n",
        "def identity_block(X, f, filters, stage, block):\n",
        "   \n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    X_shortcut = X\n",
        "   \n",
        "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    X = Add()([X, X_shortcut])# SKIP Connection\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "def convolutional_block(X, f, filters, stage, block, s=2):\n",
        "   \n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    X_shortcut = X\n",
        "\n",
        "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "def ResNet50(input_shape=(224, 224, 3)):\n",
        "\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    X = X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    X = AveragePooling2D(pool_size=(2, 2), padding='same')(X)\n",
        "    \n",
        "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
        "\n",
        "    return model\n",
        "base_model = ResNet50(input_shape=(300, 15, 1))\n",
        "headModel = base_model.output\n",
        "headModel = Flatten()(headModel)\n",
        "headModel=Dense(256, activation='relu', name='fc1',kernel_initializer=glorot_uniform(seed=0))(headModel)\n",
        "headModel=Dense(128, activation='relu', name='fc2',kernel_initializer=glorot_uniform(seed=0))(headModel)\n",
        "headModel = Dense( 14,activation='sigmoid', name='fc3',kernel_initializer=glorot_uniform(seed=0))(headModel)\n",
        "model = Model(inputs=base_model.input, outputs=headModel)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-myhbCnWAOfN"
      },
      "source": [
        "#get the gpu name \n",
        "device_name = tf.test.gpu_device_name() #you can ignore this if don't have to use the gpu during the training proccess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktl1zGNa_lwP"
      },
      "source": [
        "opt = Adam(lr=0.001)\n",
        "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "with tf.device(device_name): #in case that you skipped the gpu device comment these two lines and uncomment the last one and run\n",
        "  hist = model.fit(X_train, Y_train, epochs=10,validation_data=(X_test,Y_test))\n",
        "#hist = model.fit(X_train, Y_train, epochs=10,validation_data=(X_test,Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTzhxGuLDxW4"
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"ResNet50.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"ResNet50.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn50iVDwEFgC"
      },
      "source": [
        "# load json and create model\n",
        "json_file = open('ResNet50.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"ResNet50.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njd_lKEiETie"
      },
      "source": [
        "#to evaluate the model\n",
        "opt = Adam(lr=0.001)\n",
        "loaded_model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "y_predict = loaded_model.predict(X_test)\n",
        "print(classification_report(Y_test.argmax(axis=1),y_predict.argmax(axis=1),target_names=test_labels_set))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}