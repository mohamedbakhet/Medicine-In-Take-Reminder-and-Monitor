{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inception-v3 naive.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIGXTU1Z0Ca0VWAsbQP7Sb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedbakhet/Medicine-In-Take-Reminder-and-Monitor/blob/master/Inception_v3_naive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF3-jat69mYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8afe3d1-3450-4ebf-968d-995f85d1b391"
      },
      "source": [
        "! pip install tensorflow\n",
        "! pip install h5py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (57.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.31.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.5.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5; python_version == \"3.7\" in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3KPM5kP_NZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a91107-edab-4a33-cece-e8e2add695c1"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "import warnings\n",
        "from keras.layers import Input\n",
        "warnings.filterwarnings('ignore')\n",
        "from keras.layers import Input\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import Sequential,models\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten,MaxPooling2D,Conv1D,MaxPooling1D,add,Activation,GlobalAveragePooling2D,BatchNormalization,ReLU,MaxPool2D,SeparableConv2D,Add,GlobalAvgPool2D\n",
        "#from keras.utils import plot_model\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import os,keras\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_DCOiBU_PQd"
      },
      "source": [
        "#load train data set\n",
        "training_data=[]\n",
        "for index,value in enumerate (os.listdir('/content/drive/MyDrive/train_data')): #path to the data set on google drive\n",
        "  print(value)\n",
        "  lis=np.load('/content/drive/MyDrive/train_data/'+value)\n",
        "  for j in lis[:1600]:\n",
        "    training_data.append([np.array(j),value[:-4]])\n",
        "shuffle(training_data)\n",
        "\n",
        "#load test data set\n",
        "test_data=[]\n",
        "for index,value in enumerate (os.listdir('/content/drive/MyDrive/train_data')):\n",
        "  print(value)\n",
        "  lis=np.load('/content/drive/MyDrive/train_data/'+value)\n",
        "  for j in lis[:400]:\n",
        "    test_data.append([np.array(j),value[:-4]])\n",
        "shuffle(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYkLqonl_ZGL"
      },
      "source": [
        "#split data into train_test & encoding target\n",
        "\n",
        "X_train = np.array([i[0] for i in training_data]).reshape(-1,300,15,1)\n",
        "Y_train = np.array([i[1] for i in training_data])\n",
        "train_labels_set = np.unique(Y_train,axis=0)\n",
        "train_labels = LabelEncoder()\n",
        "train_labels = train_labels.fit(Y_train)\n",
        "Y_train = to_categorical(train_labels.transform(Y_train))\n",
        "\n",
        "X_test = np.array([i[0] for i in test_data]).reshape(-1,300,15,1)\n",
        "Y_test = np.array([i[1] for i in test_data])\n",
        "test_labels_set = np.unique(Y_test,axis=0)\n",
        "test_labels = LabelEncoder()\n",
        "test_labels = test_labels.fit(Y_test)\n",
        "Y_test = to_categorical(test_labels.transform(Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkUipjV2AAG_"
      },
      "source": [
        "#Inception-v3 naive\n",
        "input_shape = Input(shape=(300, 15, 1))\n",
        "\n",
        "### 1st layer\n",
        "layer_1 = Conv2D(10, (1,1), padding='same', activation='relu')(input_shape)\n",
        "layer_1 = Conv2D(10, (3,3), padding='same', activation='relu')(layer_1)\n",
        "\n",
        "layer_2 = Conv2D(10, (1,1), padding='same', activation='relu')(input_shape)\n",
        "layer_2 = Conv2D(10, (5,5), padding='same', activation='relu')(layer_2)\n",
        "\n",
        "layer_3 = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_shape)\n",
        "layer_3 = Conv2D(10, (1,1), padding='same', activation='relu')(layer_3)\n",
        "\n",
        "mid_1 = tf.keras.layers.concatenate([layer_1, layer_2, layer_3], axis = 3)\n",
        "\n",
        "flat_1 = Flatten()(mid_1)\n",
        "\n",
        "dense_1 = Dense(512, activation='relu')(flat_1)\n",
        "dense_2 = Dense(256, activation='relu')(dense_1)\n",
        "dense_3 = Dense(128, activation='relu')(dense_2)\n",
        "dense_4 = Dense(64, activation='relu')(dense_3)\n",
        "output = Dense(14, activation='softmax')(dense_4)\n",
        "\n",
        "model = Model([input_shape], output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKKDY6IxANfX"
      },
      "source": [
        "#get the gpu name \n",
        "device_name = tf.test.gpu_device_name() #you can ignore this if don't have to use the gpu during the training proccess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvGaRBEm_uRU"
      },
      "source": [
        "opt = Adam(lr=0.001)\n",
        "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "with tf.device(device_name): #in case that you skipped the gpu device comment these two lines and uncomment the last one and run\n",
        "  hist = model.fit(X_train, Y_train, epochs=10,validation_data=(X_test,Y_test))\n",
        "#hist = model.fit(X_train, Y_train, epochs=10,validation_data=(X_test,Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeX_ECSLD7zM"
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"Inception-v3 naive.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"Inception-v3 naive.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkEwxh55EEny"
      },
      "source": [
        "# load json and create model\n",
        "json_file = open('Inception-v3 naive.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"Inception-v3 naive.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYT8x1ZfEqD7"
      },
      "source": [
        "#to evaluate the model\n",
        "opt = Adam(lr=0.001)\n",
        "loaded_model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "y_predict = loaded_model.predict(X_test)\n",
        "print(classification_report(Y_test.argmax(axis=1),y_predict.argmax(axis=1),target_names=test_labels_set))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}